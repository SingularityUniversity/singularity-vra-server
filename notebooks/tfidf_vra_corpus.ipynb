{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VRA Corpus TFIDF\n",
    "This notebook does some pre-processing of the VRA corpus content and then applies the tf-idf algorithm to it (using n-grams of length one to three) to generate potential categories for use in tagging and searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import itertools\n",
    "import re\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "import nltk\n",
    "import pickle\n",
    "from nltk.stem import PorterStemmer\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from scipy.sparse import csr_matrix, lil_matrix\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import display, HTML\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from stopwords import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "stopwords = set(itertools.chain(*[stopwords, ENGLISH_STOP_WORDS]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conn = psycopg2.connect('dbname=vra user=postgres')\n",
    "cur = conn.cursor()\n",
    "cur.execute('select extract from core_content')\n",
    "docs = cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pre_process_text(input_text):\n",
    "    text=input_text\n",
    "    text = re.sub(r'\\b\\d(?!d)(\\w{1,4})?\\b', '', text, flags=re.I)\n",
    "    text = re.sub(r'\\b\\d{2,9}(\\w{2,4}|s|S)?\\b', '', text)\n",
    "    text = re.sub(r'\\b\\d{16}\\b', '', text)\n",
    "    text = re.sub(r'\\b0x\\w+\\b', '', text)\n",
    "    text = re.sub(r'\\b\\d+\\w(\\d|\\w)+\\b', '', text)\n",
    "    text = re.sub(r':|\\-|\\.|,|\"|\\(|\\)|\\?|\\!', '', text)\n",
    "\n",
    "    text_without_stopwords = []\n",
    "    for word in text.split():\n",
    "        if word not in stopwords:\n",
    "            word = re.sub(\"(\\w+)'s?$\", '\\g<1>', word)\n",
    "            text_without_stopwords.append(wnl.lemmatize(word.lower()))\n",
    "    text = ' '.join(text_without_stopwords)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2140/2140 [00:28<00:00, 74.21it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "should older people demen\n",
      "aggies building campus co\n",
      "machine learning theory c\n",
      "old scientist how compute\n",
      "cosmic dust earth reveals\n",
      "spacex dragon departs spa\n",
      "first global mercury map \n",
      "google reveals new androi\n",
      "nasa soundingrocket missi\n",
      "altering robot gender soc\n",
      "person interest final sea\n",
      "hirise | valley network a\n",
      "supersalty turkish lake h\n",
      "jason orbit update januar\n",
      "nasa stardust sample retu\n",
      "new detail cere seen dawn\n",
      "discovery fundamental lim\n",
      "google 'family share pinc\n",
      "bionic spaniel help super\n",
      "pentagon unveils mindcont\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "for index, item in tqdm(enumerate(docs), total=len(docs)):\n",
    "    text = '{} {}'.format(item[0]['title'], item[0]['content'])\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    text = pre_process_text(text)\n",
    "    \n",
    "    corpus.append(text)\n",
    "for item in corpus[:20]:\n",
    "    print(item[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"3d anaglyph easy\": 7.975881108029737,\n",
      "  \"2d code\": 7.975881108029737,\n",
      "  \"1d\": 7.282733927469792,\n",
      "  \"2d film\": 7.975881108029737,\n",
      "  \"2d film moment\": 7.975881108029737,\n",
      "  \"3d app\": 7.975881108029737,\n",
      "  \"2d session\": 7.570415999921573,\n",
      "  \"2d symbology data\": 7.282733927469792,\n",
      "  \"3d bioprinted brain\": 7.975881108029737,\n",
      "  \"3d bioprinter\": 7.570415999921573,\n",
      "  \"3d\": 4.125733506319679,\n",
      "  \"2d surface balloon\": 7.975881108029737,\n",
      "  \"1d 2d\": 7.570415999921573,\n",
      "  \"3d bioprinted windpipe\": 7.975881108029737,\n",
      "  \"3d bioprinted medical\": 7.975881108029737,\n",
      "  \"2d rover image\": 7.975881108029737,\n",
      "  \"2d symbol\": 7.975881108029737,\n",
      "  \"3d action platformer\": 7.975881108029737,\n",
      "  \"3d app android\": 7.975881108029737,\n",
      "  \"2d metaphor expansion\": 7.975881108029737,\n",
      "  \"1d 2d symbol\": 7.975881108029737,\n",
      "  \"2d metaphor\": 7.975881108029737,\n",
      "  \"1d south marked\": 7.975881108029737,\n",
      "  \"3d bioprinted\": 7.059590376155582,\n",
      "  \"2d code code\": 7.975881108029737,\n",
      "  \"2d na\": 7.975881108029737,\n",
      "  \"1d barcodes plus\": 7.975881108029737,\n",
      "  \"2d\": 6.184121638801683,\n",
      "  \"2d na dline\": 7.975881108029737,\n",
      "  \"2d session title\": 7.570415999921573,\n",
      "  \"1d 2d code\": 7.975881108029737,\n",
      "  \"2d surface\": 7.975881108029737,\n",
      "  \"3d bioprinted tissue\": 7.975881108029737,\n",
      "  \"3d action\": 7.975881108029737,\n",
      "  \"3d anaglyph\": 7.975881108029737,\n",
      "  \"1d south\": 7.975881108029737,\n",
      "  \"2d piece paper\": 7.975881108029737,\n",
      "  \"3d bioprinter developed\": 7.975881108029737,\n",
      "  \"2d material\": 7.975881108029737,\n",
      "  \"2d material promising\": 7.975881108029737,\n",
      "  \"3d bioprinter suddenly\": 7.975881108029737,\n",
      "  \"2d symbol xmode\": 7.975881108029737,\n",
      "  \"3d bioprinter question\": 7.975881108029737,\n",
      "  \"2d piece\": 7.975881108029737,\n",
      "  \"1d barcodes\": 7.975881108029737,\n",
      "  \"2d symbology\": 7.282733927469792,\n",
      "  \"3d bioprinted 3d\": 7.975881108029737,\n",
      "  \"2d surface completely\": 7.975881108029737,\n",
      "  \"2d rover\": 7.975881108029737,\n",
      "  \"3d bioprinted solution\": 7.975881108029737\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=1, analyzer='word', ngram_range=(1,3), stop_words='english', sublinear_tf=True)\n",
    "corpus_tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "print(json.dumps(dict(zip(vectorizer.get_feature_names()[:50], vectorizer.idf_[:50])), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2140x940159 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1417973 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('../data/ontology/readable_graph.p','rb') as infile:\n",
    "    ontology_raw = pickle.load(infile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "617752"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ontology_raw.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "423757"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names().index('internet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "def pre_process_text_with_stemming(input_text):\n",
    "    text=input_text\n",
    "    text = re.sub(r'\\b\\d(?!d)(\\w{1,4})?\\b', '', text, flags=re.I)\n",
    "    text = re.sub(r'\\b\\d{2,9}(\\w{2,4}|s|S)?\\b', '', text)\n",
    "    text = re.sub(r'\\b\\d{16}\\b', '', text)\n",
    "    text = re.sub(r'\\b0x\\w+\\b', '', text)\n",
    "    text = re.sub(r'\\b\\d+\\w(\\d|\\w)+\\b', '', text)\n",
    "    text = re.sub(r':|\\-|\\.|,|\"|\\(|\\)|\\?|\\!', '', text)\n",
    "\n",
    "    text_without_stopwords = []\n",
    "    for word in text.split():\n",
    "        if word not in stopwords:\n",
    "            word = re.sub(\"(\\w+)'s?$\", '\\g<1>', word)\n",
    "            text_without_stopwords.append(stemmer.stem(word.lower()))\n",
    "    text = ' '.join(text_without_stopwords)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing feature names\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 940159/940159 [01:40<00:00, 9318.86it/s] \n",
      "1745it [00:00, 8642.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing ontology names\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "617752it [00:56, 10872.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating tfidf_ontology_matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 103875/940159 [00:00<00:01, 518635.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(940159, 617752)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 940159/940159 [00:01<00:00, 601739.96it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"Pre-processing feature names\")\n",
    "feature_names = [pre_process_text_with_stemming(x) for x in tqdm(vectorizer.get_feature_names())]\n",
    "\n",
    "print(\"Pre-processing ontology names\")\n",
    "ontology_terms = list(ontology_raw.keys())\n",
    "ontology_terms_position_dict = {\n",
    "    pre_process_text_with_stemming(key):position for (position, key) in tqdm(enumerate(ontology_terms))\n",
    "}\n",
    "\n",
    "print(\"Creating tfidf_ontology_matrix\")\n",
    "tfidf_ontology_matrix = lil_matrix((len(feature_names), len(ontology_terms)))\n",
    "print(tfidf_ontology_matrix.shape)\n",
    "for (tfidf_index, tfidf_term) in tqdm(enumerate(feature_names), total=len(feature_names)):\n",
    "    ontology_index = ontology_terms_position_dict.get(tfidf_term, -1)\n",
    "    if ontology_index >= 0:\n",
    "        tfidf_ontology_matrix[tfidf_index,ontology_index] = 1\n",
    "        \n",
    "    #if tfidf_term in ontology_terms_set:\n",
    "    #    ontology_index = ontology_terms.index(tfidf_term)\n",
    "    #    tfidf_ontology_matrix[tfidf_index,ontology_index] = 1\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37707"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_ontology_matrix.getnnz()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(940159, 617752)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'internet'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tfidf_ontology_matrix.shape)\n",
    "feature_names[423757]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus_ontology_matrix = corpus_tfidf_matrix * tfidf_ontology_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " array([483437, 462015, 608269, 149709, 145111, 418135, 617401, 207810,\n",
       "        488539, 455657, 467368, 544157, 574729, 588664, 600281, 273693,\n",
       "        602460, 610024, 616517, 595727, 500404, 454594, 593067, 532825,\n",
       "        563607, 544006, 607533, 438541, 604547, 555662, 597508, 245081,\n",
       "        579413, 617642, 495285, 576253, 471280, 231760, 462142, 607244,\n",
       "        455422, 200286, 526807, 508296, 472904, 582915, 580326, 560200,\n",
       "        459518, 577836, 425751, 608884, 493397, 585050, 546947, 467357,\n",
       "        607095, 614369, 473328, 212577, 585520, 491300, 457752, 563592,\n",
       "        540314, 556732, 579561, 581142, 551092, 562275, 573681, 560302,\n",
       "        617234, 609098, 591258, 560175, 574812, 588235, 606757, 377910,\n",
       "        578666, 455253, 177093, 397007, 418963,  36591, 595987, 466950,\n",
       "        300347, 360995, 608530, 608710, 179914, 576778, 332088, 381044,\n",
       "        543433, 158100, 238669, 538271, 591742, 611163, 600409, 295895,\n",
       "        596843,  53505, 594401, 201162, 574060, 614848, 612883, 609336,\n",
       "        486065, 566123, 567006, 463238, 602381, 587641, 448834, 563918,\n",
       "        564345, 597570, 427517, 525195, 596764, 212886, 595346, 404682,\n",
       "        545301, 522899, 615621, 608806, 492196,  91965, 481736, 379420,\n",
       "        592791,  85618,  17493, 422793, 600793, 452969, 614710, 574861,\n",
       "        571391,  48685, 617105, 598179, 232426, 609513, 613890,  43988,\n",
       "        598624, 228757, 595917, 602866, 537191, 417513, 266642, 503246,\n",
       "        566378, 480044, 364450, 347764, 612096, 609323, 541825, 574327,\n",
       "        366117, 485583, 592149, 333065, 406361, 560067, 597934, 308601,\n",
       "        596520, 479306, 479335, 193344, 378169, 369723,  48967, 495955,\n",
       "        611091,   5105, 168236, 572942, 568159, 441133, 599065, 523818,\n",
       "         77900, 372031, 552224,  18902,  90670, 611738, 426432, 615741,\n",
       "        611527, 590293, 613738, 603492, 600363, 604250, 476468, 240054,\n",
       "        289975, 581087, 394831, 411461, 460999,  81634, 403911,  64549,\n",
       "        520003, 597474, 559893, 459736, 546506,  20309, 532408, 581414,\n",
       "        507454, 617480, 394965, 498880, 226106, 294636, 108746, 603551,\n",
       "        457601, 346040, 519056, 229943, 450716, 579543, 574312, 356764,\n",
       "        539714, 525951, 465799, 144647, 301404, 609913, 188362,  32269,\n",
       "        598599, 265540, 581953, 444839, 496712, 522159, 552580,  78901,\n",
       "        321754, 527663, 110739, 248337, 593813,  69770, 260608, 353273], dtype=int32))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nonzero(corpus_ontology_matrix[123])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.011392203991071352"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_ontology_matrix[123,483437]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back Matter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "doc_number = 1300\n",
    "keywords = vectorizer.transform([corpus[doc_number]])\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "top_keywords_data_indices = np.fliplr([keywords.data.argsort()])[0]\n",
    "for index in top_keywords_data_indices:\n",
    "    print('{}: {}'.format(feature_names[keywords.indices[index]], keywords[0, keywords.indices[index]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display(HTML(docs[doc_number][0]['content']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_tokenized = nltk.word_tokenize(docs[doc_number][0]['content'])\n",
    "processed_tokenized = nltk.word_tokenize(corpus[doc_number])\n",
    "text_processed = nltk.Text(processed_tokenized)\n",
    "text_processed_fd = nltk.FreqDist(text_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_processed_fd.most_common(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
